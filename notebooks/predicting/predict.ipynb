{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cafaad1-65de-4297-84da-066ba70a5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.read_csv('../data/processed/merged_df.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59012e3-8a95-4a31-8e44-4c391f0f2785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename \n",
    "train_df=merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eee7ae-ceb5-4f4d-be9b-9607b1981b54",
   "metadata": {},
   "source": [
    "# Reduce the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc43e44-8e59-4a52-b5ba-a5bc7b0336d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage: 28997.45 MB\n",
      "Reduced Memory Usage: 1261.80 MB\n",
      "Memory Reduced by: 95.65%\n"
     ]
    }
   ],
   "source": [
    "# fill missing values\n",
    "# Fill missing values in 'column 1' and 'column 2' with 0\n",
    "train_df['sell_price'].fillna(0, inplace=True)\n",
    "train_df['revenue'].fillna(0, inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MemoryReducer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def reduce_memory_usage(self):\n",
    "        initial_memory = self.df.memory_usage(deep=True).sum() / (1024 ** 2)  # in megabytes\n",
    "        print(f\"Initial Memory Usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "        for col in self.df.columns:\n",
    "            col_type = self.df[col].dtype\n",
    "\n",
    "            if col_type != object:\n",
    "                if \"int\" in str(col_type):\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], downcast=\"integer\")\n",
    "                elif \"float\" in str(col_type):\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], downcast=\"float\")\n",
    "            else:\n",
    "                num_unique_values = len(self.df[col].unique())\n",
    "                num_total_values = len(self.df[col])\n",
    "                if num_unique_values / num_total_values < 0.5:\n",
    "                    self.df[col] = self.df[col].astype(\"category\")\n",
    "\n",
    "        reduced_memory = self.df.memory_usage(deep=True).sum() / (1024 ** 2)  # in megabytes\n",
    "        print(f\"Reduced Memory Usage: {reduced_memory:.2f} MB\")\n",
    "        reduction_percentage = ((initial_memory - reduced_memory) / initial_memory) * 100\n",
    "        print(f\"Memory Reduced by: {reduction_percentage:.2f}%\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "reducer = MemoryReducer(train_df)\n",
    "reduced_df = reducer.reduce_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1858910d-0838-43b7-8e5d-f51e8736d598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename\n",
    "df_train=reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678358b-16a3-4d13-854e-d10270262612",
   "metadata": {},
   "source": [
    "# Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25057377-95ac-46f9-9552-718dc13564f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>revenue</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id   \n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day  count        date  wm_yr_wk event_name event_type   \n",
       "0       CA   29      0  2011-01-29     11101          0          0  \\\n",
       "1       CA   29      0  2011-01-29     11101          0          0   \n",
       "2       CA   29      0  2011-01-29     11101          0          0   \n",
       "3       CA   29      0  2011-01-29     11101          0          0   \n",
       "4       CA   29      0  2011-01-29     11101          0          0   \n",
       "\n",
       "   sell_price  revenue  year  month  \n",
       "0         0.0      0.0  2011      1  \n",
       "1         0.0      0.0  2011      1  \n",
       "2         0.0      0.0  2011      1  \n",
       "3         0.0      0.0  2011      1  \n",
       "4         0.0      0.0  2011      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b5baab-997f-43f1-b370-265fb5fe0489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47107050, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dimension\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff243b73-72e6-45ba-86fd-cc220c0ad806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2011 2012 2013 2014 2015]\n"
     ]
    }
   ],
   "source": [
    "# check unique values of year\n",
    "unique_year_values = df_train['year'].unique()\n",
    "\n",
    "# This will give you an array of unique values in the 'yar' column\n",
    "print(unique_year_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881a3c2-21f5-4d32-ae34-3c742265454a",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e0cc57-c688-4184-8126-3e0219b7f0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#binary encoding\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# Create a BinaryEncoder instance\n",
    "encoder = ce.BinaryEncoder(cols=['event_type', 'event_name'])\n",
    "\n",
    "# Fit and transform the DataFrame to perform binary encoding\n",
    "df_encoded = encoder.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3766c46c-ed82-4a0f-9763-f614df6e59ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/df_encoded.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "# save encoded\n",
    "dump(df_encoded, '../models/df_encoded.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91ab8d4-7880-4798-b155-1e3821b0c7c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename_df\n",
    "df_train=df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3847506c-76c2-4b1f-a3c4-eb9f21c67476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "# Fill missing values in 'column 1' and 'column 2' with 0\n",
    "df_train['sell_price'].fillna(0, inplace=True)\n",
    "df_train['revenue'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32579027-0d4f-4c4e-b32c-7a1757ebfd7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day',\n",
       "       'count', 'date', 'wm_yr_wk', 'event_name_0', 'event_name_1',\n",
       "       'event_name_2', 'event_name_3', 'event_name_4', 'event_type_0',\n",
       "       'event_type_1', 'event_type_2', 'sell_price', 'revenue', 'year',\n",
       "       'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the columns\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa21177a-b440-4bbd-be85-6fea5dd0bd5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop 'wm_yr_wk' column\n",
    "df_train.drop(columns=['wm_yr_wk'], inplace=True)\n",
    "\n",
    "# drop 'date' column\n",
    "df_train.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a6e10aa-e20c-48e3-8de7-a672b65c8217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47107050 entries, 0 to 47107049\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Dtype   \n",
      "---  ------        -----   \n",
      " 0   id            category\n",
      " 1   item_id       category\n",
      " 2   dept_id       category\n",
      " 3   cat_id        category\n",
      " 4   store_id      category\n",
      " 5   state_id      category\n",
      " 6   day           int8    \n",
      " 7   count         int16   \n",
      " 8   event_name_0  int64   \n",
      " 9   event_name_1  int64   \n",
      " 10  event_name_2  int64   \n",
      " 11  event_name_3  int64   \n",
      " 12  event_name_4  int64   \n",
      " 13  event_type_0  int64   \n",
      " 14  event_type_1  int64   \n",
      " 15  event_type_2  int64   \n",
      " 16  sell_price    float32 \n",
      " 17  revenue       float32 \n",
      " 18  year          int16   \n",
      " 19  month         int8    \n",
      "dtypes: category(6), float32(2), int16(2), int64(8), int8(2)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "#get info\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6445fe7f-6164-4209-bfef-c136141a4259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#calculating lags feature\n",
    "lags = [1,7,14,30]\n",
    "for lag in lags:\n",
    "   df_train[\"lag_\" + str(lag)] = df_train.groupby(\"id\")[\"revenue\"].shift(lag).astype(np.float16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28e0c5-a177-4704-8a06-3434ce8bb088",
   "metadata": {},
   "source": [
    "In summary, calculating lags is a way to look at past sales data to see if it has any influence on the sales made on the current day. It helps you explore whether there's a relationship between recent sales and today's sales in a store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d8bf792-07e2-4b99-be56-ec1b06cafdfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop id column\n",
    "df_train.drop(columns=['id'], inplace=True)\n",
    "df_train.drop(columns=['state_id'], inplace=True)\n",
    "df_train.drop(columns=['dept_id'], inplace=True)\n",
    "df_train.drop(columns=['sell_price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb0c69f-827e-4cf9-b878-eb3b741b751e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47107050 entries, 0 to 47107049\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Dtype   \n",
      "---  ------        -----   \n",
      " 0   item_id       category\n",
      " 1   cat_id        category\n",
      " 2   store_id      category\n",
      " 3   day           int8    \n",
      " 4   count         int16   \n",
      " 5   event_name_0  int64   \n",
      " 6   event_name_1  int64   \n",
      " 7   event_name_2  int64   \n",
      " 8   event_name_3  int64   \n",
      " 9   event_name_4  int64   \n",
      " 10  event_type_0  int64   \n",
      " 11  event_type_1  int64   \n",
      " 12  event_type_2  int64   \n",
      " 13  revenue       float32 \n",
      " 14  year          int16   \n",
      " 15  month         int8    \n",
      " 16  lag_1         float16 \n",
      " 17  lag_7         float16 \n",
      " 18  lag_14        float16 \n",
      " 19  lag_30        float16 \n",
      "dtypes: category(3), float16(4), float32(1), int16(2), int64(8), int8(2)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be87c0-ec32-45f9-9a75-b170b8b96400",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e7c5b8-aba4-42d8-bd84-d45a328eaeb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "train_df = df_train[df_train['year'].isin([2014, 2015])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e248882-be2a-440e-a2e8-0372f706dd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indicating x and y columns \n",
    "y= train_df[\"revenue\"]\n",
    "x= train_df.drop([\"revenue\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ecc67a2-2718-4a62-bf0e-d9e53c5db0de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Split the dataset into 2 different sets: data (80%) and test (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3187c-d609-497a-a051-91997a812184",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c180764-aed0-4882-873d-97e79f0ebc2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=250; total time=  23.9s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=250; total time=  23.3s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275425 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=250; total time=  23.8s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.282903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=250; total time=  23.5s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.271739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=250; total time=  23.6s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.256075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=350; total time=27.8min\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.298732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=350; total time=  31.3s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.534755 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=350; total time=  32.4s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.264946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=350; total time=  32.5s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.245914 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=350; total time=  31.6s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.270403 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=450; total time=  40.2s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.271932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=450; total time=  39.6s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.270218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=450; total time=  40.1s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.273908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=450; total time=  39.9s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.270541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=450; total time=  40.0s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.327893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 11586200, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466356\n",
      "Best hyperparameters: {'n_estimators': 450}\n",
      "Test RMSE: 0.7703619343145441\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize LightGBM regressor\n",
    "lgb_reg = lgb.LGBMRegressor()\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [250, 350, 450],\n",
    "}\n",
    "\n",
    "# Define K-fold cross-validation (you can use KFold for regression)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Categorical feature names - specify the names of your categorical columns\n",
    "categorical_features = ['item_id', 'cat_id', 'store_id']  \n",
    "  \n",
    "    \n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid = GridSearchCV(lgb_reg, param_dist, cv=cv, refit=True, scoring='neg_mean_absolute_error', verbose=2)\n",
    "\n",
    "# Add categorical_feature parameter to the fit function\n",
    "grid.fit(x_train, y_train, categorical_feature=categorical_features)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a5ade-a771-476a-99eb-d57f8202f8da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
