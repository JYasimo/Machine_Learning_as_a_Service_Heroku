{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ef9e4a-ae3a-45fe-a7a3-05850ef59f11",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d07c82a-fb58-401a-9562-f6b2080b4f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read data\n",
    "import pandas as pd\n",
    "merged_df = pd.read_csv('../data/processed/merged_df.csv',low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c995c2-3a54-4c70-9cdb-11ca286dfb9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename \n",
    "train_df=merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caec60c-6732-4d42-921f-7da6b9c65712",
   "metadata": {},
   "source": [
    "# Reduce the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6605135-0567-49f3-ae8d-7e5464fab074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory Usage: 28997.45 MB\n",
      "Reduced Memory Usage: 1261.80 MB\n",
      "Memory Reduced by: 95.65%\n"
     ]
    }
   ],
   "source": [
    "# fill missing values\n",
    "# Fill missing values in 'column 1' and 'column 2' with 0\n",
    "train_df['sell_price'].fillna(0, inplace=True)\n",
    "train_df['revenue'].fillna(0, inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class MemoryReducer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "\n",
    "    def reduce_memory_usage(self):\n",
    "        initial_memory = self.df.memory_usage(deep=True).sum() / (1024 ** 2)  # in megabytes\n",
    "        print(f\"Initial Memory Usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "        for col in self.df.columns:\n",
    "            col_type = self.df[col].dtype\n",
    "\n",
    "            if col_type != object:\n",
    "                if \"int\" in str(col_type):\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], downcast=\"integer\")\n",
    "                elif \"float\" in str(col_type):\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], downcast=\"float\")\n",
    "            else:\n",
    "                num_unique_values = len(self.df[col].unique())\n",
    "                num_total_values = len(self.df[col])\n",
    "                if num_unique_values / num_total_values < 0.5:\n",
    "                    self.df[col] = self.df[col].astype(\"category\")\n",
    "\n",
    "        reduced_memory = self.df.memory_usage(deep=True).sum() / (1024 ** 2)  # in megabytes\n",
    "        print(f\"Reduced Memory Usage: {reduced_memory:.2f} MB\")\n",
    "        reduction_percentage = ((initial_memory - reduced_memory) / initial_memory) * 100\n",
    "        print(f\"Memory Reduced by: {reduction_percentage:.2f}%\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "reducer = MemoryReducer(train_df)\n",
    "reduced_df = reducer.reduce_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a02a4e8-f81a-419f-86e1-9c679c7d419a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename\n",
    "df_train=reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672c9eb-fd6b-4eda-937c-1d201fefa6fc",
   "metadata": {},
   "source": [
    "# Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d393de1-d043-454d-9bab-70fa8c008e40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sell_price</th>\n",
       "      <th>revenue</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id   \n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1  \\\n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day  count        date  wm_yr_wk event_name event_type   \n",
       "0       CA   29      0  2011-01-29     11101          0          0  \\\n",
       "1       CA   29      0  2011-01-29     11101          0          0   \n",
       "2       CA   29      0  2011-01-29     11101          0          0   \n",
       "3       CA   29      0  2011-01-29     11101          0          0   \n",
       "4       CA   29      0  2011-01-29     11101          0          0   \n",
       "\n",
       "   sell_price  revenue  year  month  \n",
       "0         0.0      0.0  2011      1  \n",
       "1         0.0      0.0  2011      1  \n",
       "2         0.0      0.0  2011      1  \n",
       "3         0.0      0.0  2011      1  \n",
       "4         0.0      0.0  2011      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the head\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da2639a-1594-4e2e-b507-35c9d8b680ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47107050, 16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check dimension\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2642d770-d95e-47b9-82cb-0e5ad960ba75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2011 2012 2013 2014 2015]\n"
     ]
    }
   ],
   "source": [
    "# check unique values of year\n",
    "unique_year_values = df_train['year'].unique()\n",
    "\n",
    "# This will give you an array of unique values in the 'yar' column\n",
    "print(unique_year_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68c216-39ed-449a-b415-dade8c3b89eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97aaac9d-5dd1-4455-9f45-eff3760c4f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#binary encoding\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# Create a BinaryEncoder instance\n",
    "encoder = ce.BinaryEncoder(cols=['event_type', 'event_name'])\n",
    "\n",
    "# Fit and transform the DataFrame to perform binary encoding\n",
    "df_encoded = encoder.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a92c36-09b8-4869-b298-863f2a2918a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#rename_df\n",
    "df_train=df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc46ab9f-f530-4c02-ba4e-ddd4e1cd3c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill missing values\n",
    "# Fill missing values in 'column 1' and 'column 2' with 0\n",
    "df_train['sell_price'].fillna(0, inplace=True)\n",
    "df_train['revenue'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e01da31-83d8-4ec0-a192-190d791b403e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day',\n",
       "       'count', 'date', 'wm_yr_wk', 'event_name_0', 'event_name_1',\n",
       "       'event_name_2', 'event_name_3', 'event_name_4', 'event_type_0',\n",
       "       'event_type_1', 'event_type_2', 'sell_price', 'revenue', 'year',\n",
       "       'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see the columns\n",
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd2f1ab-f1f4-4c97-8602-f9c3d8af6c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop 'wm_yr_wk' column\n",
    "df_train.drop(columns=['wm_yr_wk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dccb466e-04e9-4796-ad38-ee8abd0d4b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop 'date' column\n",
    "df_train.drop(columns=['date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93a4484-b731-45ac-a9f3-1004fc4ab0be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47107050 entries, 0 to 47107049\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Dtype   \n",
      "---  ------        -----   \n",
      " 0   id            category\n",
      " 1   item_id       category\n",
      " 2   dept_id       category\n",
      " 3   cat_id        category\n",
      " 4   store_id      category\n",
      " 5   state_id      category\n",
      " 6   day           int8    \n",
      " 7   count         int16   \n",
      " 8   event_name_0  int64   \n",
      " 9   event_name_1  int64   \n",
      " 10  event_name_2  int64   \n",
      " 11  event_name_3  int64   \n",
      " 12  event_name_4  int64   \n",
      " 13  event_type_0  int64   \n",
      " 14  event_type_1  int64   \n",
      " 15  event_type_2  int64   \n",
      " 16  sell_price    float32 \n",
      " 17  revenue       float32 \n",
      " 18  year          int16   \n",
      " 19  month         int8    \n",
      "dtypes: category(6), float32(2), int16(2), int64(8), int8(2)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "#get info\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d0b954-fd4f-4032-ba98-d8c04691012d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#calculating lags feature\n",
    "lags = [1,7,14,30]\n",
    "for lag in lags:\n",
    "   df_train[\"lag_\" + str(lag)] = df_train.groupby(\"id\")[\"revenue\"].shift(lag).astype(np.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f415d0f-bf76-4655-bb15-9515527d5fbb",
   "metadata": {},
   "source": [
    "In summary, calculating lags is a way to look at past sales data to see if it has any influence on the sales made on the current day. It helps you explore whether there's a relationship between recent sales and today's sales in a store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63db8a73-c27a-4f1b-863e-34b6e454ae1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop id column\n",
    "df_train.drop(columns=['id'], inplace=True)\n",
    "df_train.drop(columns=['state_id'], inplace=True)\n",
    "df_train.drop(columns=['dept_id'], inplace=True)\n",
    "df_train.drop(columns=['sell_price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "207993c0-8d16-4290-81dd-cef97b332593",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47107050 entries, 0 to 47107049\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Dtype   \n",
      "---  ------        -----   \n",
      " 0   item_id       category\n",
      " 1   cat_id        category\n",
      " 2   store_id      category\n",
      " 3   day           int8    \n",
      " 4   count         int16   \n",
      " 5   event_name_0  int64   \n",
      " 6   event_name_1  int64   \n",
      " 7   event_name_2  int64   \n",
      " 8   event_name_3  int64   \n",
      " 9   event_name_4  int64   \n",
      " 10  event_type_0  int64   \n",
      " 11  event_type_1  int64   \n",
      " 12  event_type_2  int64   \n",
      " 13  revenue       float32 \n",
      " 14  year          int16   \n",
      " 15  month         int8    \n",
      " 16  lag_1         float16 \n",
      " 17  lag_7         float16 \n",
      " 18  lag_14        float16 \n",
      " 19  lag_30        float16 \n",
      "dtypes: category(3), float16(4), float32(1), int16(2), int64(8), int8(2)\n",
      "memory usage: 3.8 GB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d450b-6c8b-4381-aa5a-be5c696bea67",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13ce0fd5-4107-484d-97a6-00f898b0765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "train_df = df_train[df_train['year'].isin([2014, 2015])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7f9d6cb-feaf-49cd-b78f-46ebe5d74a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indicating x and y columns \n",
    "y= train_df[\"revenue\"]\n",
    "x= train_df.drop([\"revenue\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f329c8c5-549c-4543-bbaa-6cc2313b67ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Split the dataset into 2 different sets: data (80%) and test (20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e38fee-529c-40cf-a3e5-758935772d49",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61b5d8a9-ac57-4ec2-8fb9-3d8102f148f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.525516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=250; total time=  24.8s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.261022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=250; total time=  23.4s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.471920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=250; total time=  24.1s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=250; total time=  23.6s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.254805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=250; total time=  23.3s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.265304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=350; total time=  32.2s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=350; total time=  31.6s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=350; total time=  32.4s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.263066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=350; total time=  32.0s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=350; total time=  31.7s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466460\n",
      "[CV] END ...................................n_estimators=450; total time=  40.0s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.269068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.465685\n",
      "[CV] END ...................................n_estimators=450; total time=  39.3s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.516674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4192\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466406\n",
      "[CV] END ...................................n_estimators=450; total time=  40.5s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.272157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4193\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.464832\n",
      "[CV] END ...................................n_estimators=450; total time=  57.8s\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.423570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4191\n",
      "[LightGBM] [Info] Number of data points in the train set: 9268960, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.468399\n",
      "[CV] END ...................................n_estimators=450; total time= 1.1min\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.928282 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4187\n",
      "[LightGBM] [Info] Number of data points in the train set: 11586200, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 3.466356\n",
      "Best hyperparameters: {'n_estimators': 450}\n",
      "Test RMSE: 0.7703619343145441\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialize LightGBM regressor\n",
    "lgb_reg = lgb.LGBMRegressor()\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [250, 350, 450],\n",
    "}\n",
    "\n",
    "# Define K-fold cross-validation (you can use KFold for regression)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Categorical feature names - specify the names of your categorical columns\n",
    "categorical_features = ['item_id', 'cat_id', 'store_id']  \n",
    "  \n",
    "    \n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid = GridSearchCV(lgb_reg, param_dist, cv=cv, refit=True, scoring='neg_mean_absolute_error', verbose=2)\n",
    "\n",
    "# Add categorical_feature parameter to the fit function\n",
    "grid.fit(x_train, y_train, categorical_feature=categorical_features)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "509cd073-7cf8-4675-ab9f-4359e6699521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val RMSE: 0.7703619343145441\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the val set\n",
    "y_pred2 = best_model.predict(x_test)\n",
    "\n",
    "# Calculate RMSE on the test set\n",
    "rmse = mean_squared_error(y_test, y_pred2, squared=False)\n",
    "print(\"val RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e6ae7e4-3ff5-4370-a46b-6c4bbdab180a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/lgb_model.joblib']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model\n",
    "import joblib\n",
    "joblib.dump(best_model, '../models/lgb_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
